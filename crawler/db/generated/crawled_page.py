# Code generated by sqlc. DO NOT EDIT.
# versions:
#   sqlc v1.29.0
# source: crawled_page.sql
import datetime
import pydantic
from typing import Any, AsyncIterator, Optional
import uuid

import sqlalchemy
import sqlalchemy.ext.asyncio

from crawler.db.generated import models


COUNT_DUPLICATE_PAGES = """-- name: count_duplicate_pages \\:one
SELECT COUNT(*) FROM crawled_page
WHERE is_duplicate = true
    AND website_id = :p1
"""


COUNT_PAGES_BY_JOB = """-- name: count_pages_by_job \\:one
SELECT COUNT(*) FROM crawled_page
WHERE job_id = :p1
"""


COUNT_PAGES_BY_WEBSITE = """-- name: count_pages_by_website \\:one
SELECT COUNT(*) FROM crawled_page
WHERE website_id = :p1
"""


CREATE_CRAWLED_PAGE = """-- name: create_crawled_page \\:one
INSERT INTO crawled_page (
    website_id,
    job_id,
    url,
    url_hash,
    content_hash,
    title,
    extracted_content,
    metadata,
    gcs_html_path,
    gcs_documents,
    crawled_at
) VALUES (
    :p1,
    :p2,
    :p3,
    :p4,
    :p5,
    :p6,
    :p7,
    :p8,
    :p9,
    :p10,
    :p11
)
RETURNING id, website_id, job_id, url, url_hash, content_hash, title, extracted_content, metadata, gcs_html_path, gcs_documents, is_duplicate, duplicate_of, similarity_score, crawled_at, created_at
"""


class CreateCrawledPageParams(pydantic.BaseModel):
    website_id: uuid.UUID
    job_id: uuid.UUID
    url: str
    url_hash: str
    content_hash: str
    title: Optional[str]
    extracted_content: Optional[str]
    metadata: Optional[Any]
    gcs_html_path: Optional[str]
    gcs_documents: Optional[Any]
    crawled_at: datetime.datetime


DELETE_OLD_PAGES = """-- name: delete_old_pages \\:exec
DELETE FROM crawled_page
WHERE crawled_at < CURRENT_TIMESTAMP - INTERVAL '90 days'
    AND website_id = :p1
"""


GET_CRAWLED_PAGE_BY_ID = """-- name: get_crawled_page_by_id \\:one
SELECT id, website_id, job_id, url, url_hash, content_hash, title, extracted_content, metadata, gcs_html_path, gcs_documents, is_duplicate, duplicate_of, similarity_score, crawled_at, created_at FROM crawled_page
WHERE id = :p1
"""


GET_DUPLICATE_PAGES = """-- name: get_duplicate_pages \\:many
SELECT id, website_id, job_id, url, url_hash, content_hash, title, extracted_content, metadata, gcs_html_path, gcs_documents, is_duplicate, duplicate_of, similarity_score, crawled_at, created_at FROM crawled_page
WHERE is_duplicate = true
    AND website_id = :p1
ORDER BY crawled_at DESC
LIMIT :p3 OFFSET :p2
"""


GET_PAGE_BY_CONTENT_HASH = """-- name: get_page_by_content_hash \\:one
SELECT id, website_id, job_id, url, url_hash, content_hash, title, extracted_content, metadata, gcs_html_path, gcs_documents, is_duplicate, duplicate_of, similarity_score, crawled_at, created_at FROM crawled_page
WHERE content_hash = :p1
ORDER BY crawled_at ASC
LIMIT 1
"""


GET_PAGE_BY_URL_HASH = """-- name: get_page_by_url_hash \\:one
SELECT id, website_id, job_id, url, url_hash, content_hash, title, extracted_content, metadata, gcs_html_path, gcs_documents, is_duplicate, duplicate_of, similarity_score, crawled_at, created_at FROM crawled_page
WHERE website_id = :p1 AND url_hash = :p2
"""


GET_PAGE_STATS = """-- name: get_page_stats \\:one
SELECT
    COUNT(*) as total_pages,
    COUNT(*) FILTER (WHERE is_duplicate = false) as unique_pages,
    COUNT(*) FILTER (WHERE is_duplicate = true) as duplicate_pages,
    AVG(similarity_score) FILTER (WHERE is_duplicate = true) as avg_similarity_score
FROM crawled_page
WHERE website_id = :p1
"""


class GetPageStatsRow(pydantic.BaseModel):
    total_pages: int
    unique_pages: int
    duplicate_pages: int
    avg_similarity_score: float


LIST_PAGES_BY_JOB = """-- name: list_pages_by_job \\:many
SELECT id, website_id, job_id, url, url_hash, content_hash, title, extracted_content, metadata, gcs_html_path, gcs_documents, is_duplicate, duplicate_of, similarity_score, crawled_at, created_at FROM crawled_page
WHERE job_id = :p1
ORDER BY crawled_at DESC
LIMIT :p3 OFFSET :p2
"""


LIST_PAGES_BY_WEBSITE = """-- name: list_pages_by_website \\:many
SELECT id, website_id, job_id, url, url_hash, content_hash, title, extracted_content, metadata, gcs_html_path, gcs_documents, is_duplicate, duplicate_of, similarity_score, crawled_at, created_at FROM crawled_page
WHERE website_id = :p1
ORDER BY crawled_at DESC
LIMIT :p3 OFFSET :p2
"""


MARK_PAGE_AS_DUPLICATE = """-- name: mark_page_as_duplicate \\:one
UPDATE crawled_page
SET
    is_duplicate = true,
    duplicate_of = :p1,
    similarity_score = :p2
WHERE id = :p3
RETURNING id, website_id, job_id, url, url_hash, content_hash, title, extracted_content, metadata, gcs_html_path, gcs_documents, is_duplicate, duplicate_of, similarity_score, crawled_at, created_at
"""


UPDATE_PAGE_CONTENT = """-- name: update_page_content \\:one
UPDATE crawled_page
SET
    title = COALESCE(:p1, title),
    extracted_content = COALESCE(:p2, extracted_content),
    metadata = COALESCE(:p3, metadata),
    gcs_html_path = COALESCE(:p4, gcs_html_path),
    gcs_documents = COALESCE(:p5, gcs_documents)
WHERE id = :p6
RETURNING id, website_id, job_id, url, url_hash, content_hash, title, extracted_content, metadata, gcs_html_path, gcs_documents, is_duplicate, duplicate_of, similarity_score, crawled_at, created_at
"""


class AsyncQuerier:
    def __init__(self, conn: sqlalchemy.ext.asyncio.AsyncConnection):
        self._conn = conn

    async def count_duplicate_pages(self, *, website_id: uuid.UUID) -> Optional[int]:
        row = (await self._conn.execute(sqlalchemy.text(COUNT_DUPLICATE_PAGES), {"p1": website_id})).first()
        if row is None:
            return None
        return row[0]

    async def count_pages_by_job(self, *, job_id: uuid.UUID) -> Optional[int]:
        row = (await self._conn.execute(sqlalchemy.text(COUNT_PAGES_BY_JOB), {"p1": job_id})).first()
        if row is None:
            return None
        return row[0]

    async def count_pages_by_website(self, *, website_id: uuid.UUID) -> Optional[int]:
        row = (await self._conn.execute(sqlalchemy.text(COUNT_PAGES_BY_WEBSITE), {"p1": website_id})).first()
        if row is None:
            return None
        return row[0]

    async def create_crawled_page(self, arg: CreateCrawledPageParams) -> Optional[models.CrawledPage]:
        row = (await self._conn.execute(sqlalchemy.text(CREATE_CRAWLED_PAGE), {
            "p1": arg.website_id,
            "p2": arg.job_id,
            "p3": arg.url,
            "p4": arg.url_hash,
            "p5": arg.content_hash,
            "p6": arg.title,
            "p7": arg.extracted_content,
            "p8": arg.metadata,
            "p9": arg.gcs_html_path,
            "p10": arg.gcs_documents,
            "p11": arg.crawled_at,
        })).first()
        if row is None:
            return None
        return models.CrawledPage(
            id=row[0],
            website_id=row[1],
            job_id=row[2],
            url=row[3],
            url_hash=row[4],
            content_hash=row[5],
            title=row[6],
            extracted_content=row[7],
            metadata=row[8],
            gcs_html_path=row[9],
            gcs_documents=row[10],
            is_duplicate=row[11],
            duplicate_of=row[12],
            similarity_score=row[13],
            crawled_at=row[14],
            created_at=row[15],
        )

    async def delete_old_pages(self, *, website_id: uuid.UUID) -> None:
        await self._conn.execute(sqlalchemy.text(DELETE_OLD_PAGES), {"p1": website_id})

    async def get_crawled_page_by_id(self, *, id: uuid.UUID) -> Optional[models.CrawledPage]:
        row = (await self._conn.execute(sqlalchemy.text(GET_CRAWLED_PAGE_BY_ID), {"p1": id})).first()
        if row is None:
            return None
        return models.CrawledPage(
            id=row[0],
            website_id=row[1],
            job_id=row[2],
            url=row[3],
            url_hash=row[4],
            content_hash=row[5],
            title=row[6],
            extracted_content=row[7],
            metadata=row[8],
            gcs_html_path=row[9],
            gcs_documents=row[10],
            is_duplicate=row[11],
            duplicate_of=row[12],
            similarity_score=row[13],
            crawled_at=row[14],
            created_at=row[15],
        )

    async def get_duplicate_pages(self, *, website_id: uuid.UUID, offset_count: int, limit_count: int) -> AsyncIterator[models.CrawledPage]:
        result = await self._conn.stream(sqlalchemy.text(GET_DUPLICATE_PAGES), {"p1": website_id, "p2": offset_count, "p3": limit_count})
        async for row in result:
            yield models.CrawledPage(
                id=row[0],
                website_id=row[1],
                job_id=row[2],
                url=row[3],
                url_hash=row[4],
                content_hash=row[5],
                title=row[6],
                extracted_content=row[7],
                metadata=row[8],
                gcs_html_path=row[9],
                gcs_documents=row[10],
                is_duplicate=row[11],
                duplicate_of=row[12],
                similarity_score=row[13],
                crawled_at=row[14],
                created_at=row[15],
            )

    async def get_page_by_content_hash(self, *, content_hash: str) -> Optional[models.CrawledPage]:
        row = (await self._conn.execute(sqlalchemy.text(GET_PAGE_BY_CONTENT_HASH), {"p1": content_hash})).first()
        if row is None:
            return None
        return models.CrawledPage(
            id=row[0],
            website_id=row[1],
            job_id=row[2],
            url=row[3],
            url_hash=row[4],
            content_hash=row[5],
            title=row[6],
            extracted_content=row[7],
            metadata=row[8],
            gcs_html_path=row[9],
            gcs_documents=row[10],
            is_duplicate=row[11],
            duplicate_of=row[12],
            similarity_score=row[13],
            crawled_at=row[14],
            created_at=row[15],
        )

    async def get_page_by_url_hash(self, *, website_id: uuid.UUID, url_hash: str) -> Optional[models.CrawledPage]:
        row = (await self._conn.execute(sqlalchemy.text(GET_PAGE_BY_URL_HASH), {"p1": website_id, "p2": url_hash})).first()
        if row is None:
            return None
        return models.CrawledPage(
            id=row[0],
            website_id=row[1],
            job_id=row[2],
            url=row[3],
            url_hash=row[4],
            content_hash=row[5],
            title=row[6],
            extracted_content=row[7],
            metadata=row[8],
            gcs_html_path=row[9],
            gcs_documents=row[10],
            is_duplicate=row[11],
            duplicate_of=row[12],
            similarity_score=row[13],
            crawled_at=row[14],
            created_at=row[15],
        )

    async def get_page_stats(self, *, website_id: uuid.UUID) -> Optional[GetPageStatsRow]:
        row = (await self._conn.execute(sqlalchemy.text(GET_PAGE_STATS), {"p1": website_id})).first()
        if row is None:
            return None
        return GetPageStatsRow(
            total_pages=row[0],
            unique_pages=row[1],
            duplicate_pages=row[2],
            avg_similarity_score=row[3],
        )

    async def list_pages_by_job(self, *, job_id: uuid.UUID, offset_count: int, limit_count: int) -> AsyncIterator[models.CrawledPage]:
        result = await self._conn.stream(sqlalchemy.text(LIST_PAGES_BY_JOB), {"p1": job_id, "p2": offset_count, "p3": limit_count})
        async for row in result:
            yield models.CrawledPage(
                id=row[0],
                website_id=row[1],
                job_id=row[2],
                url=row[3],
                url_hash=row[4],
                content_hash=row[5],
                title=row[6],
                extracted_content=row[7],
                metadata=row[8],
                gcs_html_path=row[9],
                gcs_documents=row[10],
                is_duplicate=row[11],
                duplicate_of=row[12],
                similarity_score=row[13],
                crawled_at=row[14],
                created_at=row[15],
            )

    async def list_pages_by_website(self, *, website_id: uuid.UUID, offset_count: int, limit_count: int) -> AsyncIterator[models.CrawledPage]:
        result = await self._conn.stream(sqlalchemy.text(LIST_PAGES_BY_WEBSITE), {"p1": website_id, "p2": offset_count, "p3": limit_count})
        async for row in result:
            yield models.CrawledPage(
                id=row[0],
                website_id=row[1],
                job_id=row[2],
                url=row[3],
                url_hash=row[4],
                content_hash=row[5],
                title=row[6],
                extracted_content=row[7],
                metadata=row[8],
                gcs_html_path=row[9],
                gcs_documents=row[10],
                is_duplicate=row[11],
                duplicate_of=row[12],
                similarity_score=row[13],
                crawled_at=row[14],
                created_at=row[15],
            )

    async def mark_page_as_duplicate(self, *, duplicate_of: Optional[uuid.UUID], similarity_score: Optional[int], id: uuid.UUID) -> Optional[models.CrawledPage]:
        row = (await self._conn.execute(sqlalchemy.text(MARK_PAGE_AS_DUPLICATE), {"p1": duplicate_of, "p2": similarity_score, "p3": id})).first()
        if row is None:
            return None
        return models.CrawledPage(
            id=row[0],
            website_id=row[1],
            job_id=row[2],
            url=row[3],
            url_hash=row[4],
            content_hash=row[5],
            title=row[6],
            extracted_content=row[7],
            metadata=row[8],
            gcs_html_path=row[9],
            gcs_documents=row[10],
            is_duplicate=row[11],
            duplicate_of=row[12],
            similarity_score=row[13],
            crawled_at=row[14],
            created_at=row[15],
        )

    async def update_page_content(self, *, title: Optional[str], extracted_content: Optional[str], metadata: Optional[Any], gcs_html_path: Optional[str], gcs_documents: Optional[Any], id: uuid.UUID) -> Optional[models.CrawledPage]:
        row = (await self._conn.execute(sqlalchemy.text(UPDATE_PAGE_CONTENT), {
            "p1": title,
            "p2": extracted_content,
            "p3": metadata,
            "p4": gcs_html_path,
            "p5": gcs_documents,
            "p6": id,
        })).first()
        if row is None:
            return None
        return models.CrawledPage(
            id=row[0],
            website_id=row[1],
            job_id=row[2],
            url=row[3],
            url_hash=row[4],
            content_hash=row[5],
            title=row[6],
            extracted_content=row[7],
            metadata=row[8],
            gcs_html_path=row[9],
            gcs_documents=row[10],
            is_duplicate=row[11],
            duplicate_of=row[12],
            similarity_score=row[13],
            crawled_at=row[14],
            created_at=row[15],
        )
