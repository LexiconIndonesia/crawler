# Code generated by sqlc. DO NOT EDIT.
# versions:
#   sqlc v1.30.0
# source: duplicate_group.sql
import datetime
import pydantic
from typing import Any, AsyncIterator, Optional
import uuid

import sqlalchemy
import sqlalchemy.ext.asyncio

from crawler.db.generated import models


ADD_DUPLICATE_RELATIONSHIP = """-- name: add_duplicate_relationship \\:one
INSERT INTO duplicate_relationship (
    group_id,
    duplicate_page_id,
    detection_method,
    similarity_score,
    confidence_threshold,
    detected_by
) VALUES (
    :p1, :p2, :p3, :p4, :p5, :p6
) RETURNING id, group_id, duplicate_page_id, detection_method, similarity_score, confidence_threshold, detected_at, detected_by
"""


COUNT_DUPLICATES_BY_METHOD = """-- name: count_duplicates_by_method \\:many
SELECT
    detection_method,
    COUNT(*) as count
FROM duplicate_relationship
GROUP BY detection_method
ORDER BY count DESC
"""


class CountDuplicatesByMethodRow(pydantic.BaseModel):
    detection_method: str
    count: int


CREATE_DUPLICATE_GROUP = """-- name: create_duplicate_group \\:one

INSERT INTO duplicate_group (
    canonical_page_id
) VALUES (
    :p1
) RETURNING id, canonical_page_id, group_size, created_at, updated_at
"""


FIND_DUPLICATE_GROUP_FOR_PAGE = """-- name: find_duplicate_group_for_page \\:one
SELECT dg.id, dg.canonical_page_id, dg.group_size, dg.created_at, dg.updated_at
FROM duplicate_group dg
JOIN duplicate_relationship dr ON dr.group_id = dg.id
WHERE dr.duplicate_page_id = :p1
"""


FIND_PAGES_WITHOUT_DUPLICATE_GROUP = """-- name: find_pages_without_duplicate_group \\:many
SELECT cp.id, cp.website_id, cp.job_id, cp.url, cp.url_hash, cp.content_hash, cp.title, cp.extracted_content, cp.metadata, cp.gcs_html_path, cp.gcs_documents, cp.is_duplicate, cp.duplicate_of, cp.similarity_score, cp.crawled_at, cp.created_at
FROM crawled_page cp
WHERE cp.is_duplicate = true
  AND NOT EXISTS (
      SELECT 1 FROM duplicate_relationship dr
      WHERE dr.duplicate_page_id = cp.id
  )
LIMIT :p1 OFFSET :p2
"""


GET_CANONICAL_PAGE_FOR_DUPLICATE = """-- name: get_canonical_page_for_duplicate \\:one
SELECT cp.id, cp.website_id, cp.job_id, cp.url, cp.url_hash, cp.content_hash, cp.title, cp.extracted_content, cp.metadata, cp.gcs_html_path, cp.gcs_documents, cp.is_duplicate, cp.duplicate_of, cp.similarity_score, cp.crawled_at, cp.created_at
FROM crawled_page cp
JOIN duplicate_group dg ON dg.canonical_page_id = cp.id
JOIN duplicate_relationship dr ON dr.group_id = dg.id
WHERE dr.duplicate_page_id = :p1
"""


GET_DUPLICATE_GROUP = """-- name: get_duplicate_group \\:one
SELECT id, canonical_page_id, group_size, created_at, updated_at FROM duplicate_group
WHERE id = :p1
"""


GET_DUPLICATE_GROUP_BY_CANONICAL_PAGE = """-- name: get_duplicate_group_by_canonical_page \\:one
SELECT id, canonical_page_id, group_size, created_at, updated_at FROM duplicate_group
WHERE canonical_page_id = :p1
"""


GET_DUPLICATE_GROUP_STATS = """-- name: get_duplicate_group_stats \\:one
SELECT
    dg.id,
    dg.canonical_page_id,
    dg.group_size,
    COUNT(dr.id) as relationship_count,
    AVG(dr.similarity_score) as avg_similarity,
    MIN(dr.detected_at) as first_detected,
    MAX(dr.detected_at) as last_detected
FROM duplicate_group dg
LEFT JOIN duplicate_relationship dr ON dr.group_id = dg.id
WHERE dg.id = :p1
GROUP BY dg.id, dg.canonical_page_id, dg.group_size
"""


class GetDuplicateGroupStatsRow(pydantic.BaseModel):
    id: uuid.UUID
    canonical_page_id: uuid.UUID
    group_size: int
    relationship_count: int
    avg_similarity: float
    first_detected: Any
    last_detected: Any


GET_DUPLICATE_RELATIONSHIP = """-- name: get_duplicate_relationship \\:one
SELECT id, group_id, duplicate_page_id, detection_method, similarity_score, confidence_threshold, detected_at, detected_by FROM duplicate_relationship
WHERE id = :p1
"""


GET_DUPLICATE_RELATIONSHIP_BY_PAGE = """-- name: get_duplicate_relationship_by_page \\:one
SELECT id, group_id, duplicate_page_id, detection_method, similarity_score, confidence_threshold, detected_at, detected_by FROM duplicate_relationship
WHERE group_id = :p1 AND duplicate_page_id = :p2
"""


GET_GROUP_WITH_CANONICAL_PAGE = """-- name: get_group_with_canonical_page \\:one
SELECT
    dg.id, dg.canonical_page_id, dg.group_size, dg.created_at, dg.updated_at,
    cp.url as canonical_url,
    cp.crawled_at as canonical_crawled_at,
    cp.content_hash as canonical_content_hash
FROM duplicate_group dg
JOIN crawled_page cp ON cp.id = dg.canonical_page_id
WHERE dg.id = :p1
"""


class GetGroupWithCanonicalPageRow(pydantic.BaseModel):
    id: uuid.UUID
    canonical_page_id: uuid.UUID
    group_size: int
    created_at: datetime.datetime
    updated_at: datetime.datetime
    canonical_url: str
    canonical_crawled_at: datetime.datetime
    canonical_content_hash: str


LIST_ALL_DUPLICATE_GROUPS = """-- name: list_all_duplicate_groups \\:many
SELECT id, canonical_page_id, group_size, created_at, updated_at FROM duplicate_group
ORDER BY created_at DESC
LIMIT :p1 OFFSET :p2
"""


LIST_DUPLICATES_IN_GROUP = """-- name: list_duplicates_in_group \\:many
SELECT
    dr.id, dr.group_id, dr.duplicate_page_id, dr.detection_method, dr.similarity_score, dr.confidence_threshold, dr.detected_at, dr.detected_by,
    cp.url,
    cp.crawled_at,
    cp.content_hash
FROM duplicate_relationship dr
JOIN crawled_page cp ON cp.id = dr.duplicate_page_id
WHERE dr.group_id = :p1
ORDER BY dr.detected_at DESC
"""


class ListDuplicatesInGroupRow(pydantic.BaseModel):
    id: int
    group_id: uuid.UUID
    duplicate_page_id: uuid.UUID
    detection_method: str
    similarity_score: Optional[int]
    confidence_threshold: Optional[int]
    detected_at: datetime.datetime
    detected_by: Optional[str]
    url: str
    crawled_at: datetime.datetime
    content_hash: str


REMOVE_DUPLICATE_GROUP = """-- name: remove_duplicate_group \\:exec
DELETE FROM duplicate_group
WHERE id = :p1
"""


REMOVE_DUPLICATE_RELATIONSHIP = """-- name: remove_duplicate_relationship \\:exec
DELETE FROM duplicate_relationship
WHERE id = :p1
"""


UPDATE_DUPLICATE_SIMILARITY_SCORE = """-- name: update_duplicate_similarity_score \\:one
UPDATE duplicate_relationship
SET similarity_score = :p2
WHERE id = :p1
RETURNING id, group_id, duplicate_page_id, detection_method, similarity_score, confidence_threshold, detected_at, detected_by
"""


class AsyncQuerier:
    def __init__(self, conn: sqlalchemy.ext.asyncio.AsyncConnection):
        self._conn = conn

    async def add_duplicate_relationship(self, *, group_id: uuid.UUID, duplicate_page_id: uuid.UUID, detection_method: str, similarity_score: Optional[int], confidence_threshold: Optional[int], detected_by: Optional[str]) -> Optional[models.DuplicateRelationship]:
        row = (await self._conn.execute(sqlalchemy.text(ADD_DUPLICATE_RELATIONSHIP), {
            "p1": group_id,
            "p2": duplicate_page_id,
            "p3": detection_method,
            "p4": similarity_score,
            "p5": confidence_threshold,
            "p6": detected_by,
        })).first()
        if row is None:
            return None
        return models.DuplicateRelationship(
            id=row[0],
            group_id=row[1],
            duplicate_page_id=row[2],
            detection_method=row[3],
            similarity_score=row[4],
            confidence_threshold=row[5],
            detected_at=row[6],
            detected_by=row[7],
        )

    async def count_duplicates_by_method(self) -> AsyncIterator[CountDuplicatesByMethodRow]:
        result = await self._conn.stream(sqlalchemy.text(COUNT_DUPLICATES_BY_METHOD))
        async for row in result:
            yield CountDuplicatesByMethodRow(
                detection_method=row[0],
                count=row[1],
            )

    async def create_duplicate_group(self, *, canonical_page_id: uuid.UUID) -> Optional[models.DuplicateGroup]:
        row = (await self._conn.execute(sqlalchemy.text(CREATE_DUPLICATE_GROUP), {"p1": canonical_page_id})).first()
        if row is None:
            return None
        return models.DuplicateGroup(
            id=row[0],
            canonical_page_id=row[1],
            group_size=row[2],
            created_at=row[3],
            updated_at=row[4],
        )

    async def find_duplicate_group_for_page(self, *, duplicate_page_id: uuid.UUID) -> Optional[models.DuplicateGroup]:
        row = (await self._conn.execute(sqlalchemy.text(FIND_DUPLICATE_GROUP_FOR_PAGE), {"p1": duplicate_page_id})).first()
        if row is None:
            return None
        return models.DuplicateGroup(
            id=row[0],
            canonical_page_id=row[1],
            group_size=row[2],
            created_at=row[3],
            updated_at=row[4],
        )

    async def find_pages_without_duplicate_group(self, *, limit: int, offset: int) -> AsyncIterator[models.CrawledPage]:
        result = await self._conn.stream(sqlalchemy.text(FIND_PAGES_WITHOUT_DUPLICATE_GROUP), {"p1": limit, "p2": offset})
        async for row in result:
            yield models.CrawledPage(
                id=row[0],
                website_id=row[1],
                job_id=row[2],
                url=row[3],
                url_hash=row[4],
                content_hash=row[5],
                title=row[6],
                extracted_content=row[7],
                metadata=row[8],
                gcs_html_path=row[9],
                gcs_documents=row[10],
                is_duplicate=row[11],
                duplicate_of=row[12],
                similarity_score=row[13],
                crawled_at=row[14],
                created_at=row[15],
            )

    async def get_canonical_page_for_duplicate(self, *, duplicate_page_id: uuid.UUID) -> Optional[models.CrawledPage]:
        row = (await self._conn.execute(sqlalchemy.text(GET_CANONICAL_PAGE_FOR_DUPLICATE), {"p1": duplicate_page_id})).first()
        if row is None:
            return None
        return models.CrawledPage(
            id=row[0],
            website_id=row[1],
            job_id=row[2],
            url=row[3],
            url_hash=row[4],
            content_hash=row[5],
            title=row[6],
            extracted_content=row[7],
            metadata=row[8],
            gcs_html_path=row[9],
            gcs_documents=row[10],
            is_duplicate=row[11],
            duplicate_of=row[12],
            similarity_score=row[13],
            crawled_at=row[14],
            created_at=row[15],
        )

    async def get_duplicate_group(self, *, id: uuid.UUID) -> Optional[models.DuplicateGroup]:
        row = (await self._conn.execute(sqlalchemy.text(GET_DUPLICATE_GROUP), {"p1": id})).first()
        if row is None:
            return None
        return models.DuplicateGroup(
            id=row[0],
            canonical_page_id=row[1],
            group_size=row[2],
            created_at=row[3],
            updated_at=row[4],
        )

    async def get_duplicate_group_by_canonical_page(self, *, canonical_page_id: uuid.UUID) -> Optional[models.DuplicateGroup]:
        row = (await self._conn.execute(sqlalchemy.text(GET_DUPLICATE_GROUP_BY_CANONICAL_PAGE), {"p1": canonical_page_id})).first()
        if row is None:
            return None
        return models.DuplicateGroup(
            id=row[0],
            canonical_page_id=row[1],
            group_size=row[2],
            created_at=row[3],
            updated_at=row[4],
        )

    async def get_duplicate_group_stats(self, *, id: uuid.UUID) -> Optional[GetDuplicateGroupStatsRow]:
        row = (await self._conn.execute(sqlalchemy.text(GET_DUPLICATE_GROUP_STATS), {"p1": id})).first()
        if row is None:
            return None
        return GetDuplicateGroupStatsRow(
            id=row[0],
            canonical_page_id=row[1],
            group_size=row[2],
            relationship_count=row[3],
            avg_similarity=row[4],
            first_detected=row[5],
            last_detected=row[6],
        )

    async def get_duplicate_relationship(self, *, id: int) -> Optional[models.DuplicateRelationship]:
        row = (await self._conn.execute(sqlalchemy.text(GET_DUPLICATE_RELATIONSHIP), {"p1": id})).first()
        if row is None:
            return None
        return models.DuplicateRelationship(
            id=row[0],
            group_id=row[1],
            duplicate_page_id=row[2],
            detection_method=row[3],
            similarity_score=row[4],
            confidence_threshold=row[5],
            detected_at=row[6],
            detected_by=row[7],
        )

    async def get_duplicate_relationship_by_page(self, *, group_id: uuid.UUID, duplicate_page_id: uuid.UUID) -> Optional[models.DuplicateRelationship]:
        row = (await self._conn.execute(sqlalchemy.text(GET_DUPLICATE_RELATIONSHIP_BY_PAGE), {"p1": group_id, "p2": duplicate_page_id})).first()
        if row is None:
            return None
        return models.DuplicateRelationship(
            id=row[0],
            group_id=row[1],
            duplicate_page_id=row[2],
            detection_method=row[3],
            similarity_score=row[4],
            confidence_threshold=row[5],
            detected_at=row[6],
            detected_by=row[7],
        )

    async def get_group_with_canonical_page(self, *, id: uuid.UUID) -> Optional[GetGroupWithCanonicalPageRow]:
        row = (await self._conn.execute(sqlalchemy.text(GET_GROUP_WITH_CANONICAL_PAGE), {"p1": id})).first()
        if row is None:
            return None
        return GetGroupWithCanonicalPageRow(
            id=row[0],
            canonical_page_id=row[1],
            group_size=row[2],
            created_at=row[3],
            updated_at=row[4],
            canonical_url=row[5],
            canonical_crawled_at=row[6],
            canonical_content_hash=row[7],
        )

    async def list_all_duplicate_groups(self, *, limit: int, offset: int) -> AsyncIterator[models.DuplicateGroup]:
        result = await self._conn.stream(sqlalchemy.text(LIST_ALL_DUPLICATE_GROUPS), {"p1": limit, "p2": offset})
        async for row in result:
            yield models.DuplicateGroup(
                id=row[0],
                canonical_page_id=row[1],
                group_size=row[2],
                created_at=row[3],
                updated_at=row[4],
            )

    async def list_duplicates_in_group(self, *, group_id: uuid.UUID) -> AsyncIterator[ListDuplicatesInGroupRow]:
        result = await self._conn.stream(sqlalchemy.text(LIST_DUPLICATES_IN_GROUP), {"p1": group_id})
        async for row in result:
            yield ListDuplicatesInGroupRow(
                id=row[0],
                group_id=row[1],
                duplicate_page_id=row[2],
                detection_method=row[3],
                similarity_score=row[4],
                confidence_threshold=row[5],
                detected_at=row[6],
                detected_by=row[7],
                url=row[8],
                crawled_at=row[9],
                content_hash=row[10],
            )

    async def remove_duplicate_group(self, *, id: uuid.UUID) -> None:
        await self._conn.execute(sqlalchemy.text(REMOVE_DUPLICATE_GROUP), {"p1": id})

    async def remove_duplicate_relationship(self, *, id: int) -> None:
        await self._conn.execute(sqlalchemy.text(REMOVE_DUPLICATE_RELATIONSHIP), {"p1": id})

    async def update_duplicate_similarity_score(self, *, id: int, similarity_score: Optional[int]) -> Optional[models.DuplicateRelationship]:
        row = (await self._conn.execute(sqlalchemy.text(UPDATE_DUPLICATE_SIMILARITY_SCORE), {"p1": id, "p2": similarity_score})).first()
        if row is None:
            return None
        return models.DuplicateRelationship(
            id=row[0],
            group_id=row[1],
            duplicate_page_id=row[2],
            detection_method=row[3],
            similarity_score=row[4],
            confidence_threshold=row[5],
            detected_at=row[6],
            detected_by=row[7],
        )
