# Code generated by sqlc. DO NOT EDIT.
# versions:
#   sqlc v1.30.0
import datetime
import enum
import pydantic
from typing import Any, Optional
import uuid


class BackoffStrategyEnum(str, enum.Enum):
    EXPONENTIAL = "exponential"
    LINEAR = "linear"
    FIXED = "fixed"


class ErrorCategoryEnum(str, enum.Enum):
    NETWORK = "network"
    RATE_LIMIT = "rate_limit"
    SERVER_ERROR = "server_error"
    BROWSER_CRASH = "browser_crash"
    RESOURCE_UNAVAILABLE = "resource_unavailable"
    TIMEOUT = "timeout"
    CLIENT_ERROR = "client_error"
    AUTH_ERROR = "auth_error"
    NOT_FOUND = "not_found"
    VALIDATION_ERROR = "validation_error"
    BUSINESS_LOGIC_ERROR = "business_logic_error"
    UNKNOWN = "unknown"


class JobTypeEnum(str, enum.Enum):
    ONE_TIME = "one_time"
    SCHEDULED = "scheduled"
    RECURRING = "recurring"


class LogLevelEnum(str, enum.Enum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"


class StatusEnum(str, enum.Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    ACTIVE = "active"
    INACTIVE = "inactive"


class AlembicVersion(pydantic.BaseModel):
    version_num: str


class ContentHash(pydantic.BaseModel):
    """Tracks content hash occurrences for duplicate detection"""
    content_hash: str
    first_seen_page_id: Optional[uuid.UUID]
    occurrence_count: int
    last_seen_at: datetime.datetime
    created_at: datetime.datetime
    # 64-bit Simhash fingerprint for fuzzy duplicate detection
    simhash_fingerprint: Optional[int]


class CrawlJob(pydantic.BaseModel):
    """Stores crawl job definitions and execution state"""
    id: uuid.UUID
    # Reference to website template (nullable for inline config jobs)
    website_id: Optional[uuid.UUID]
    job_type: JobTypeEnum
    seed_url: str
    # Inline configuration for jobs without website template
    inline_config: Optional[Any]
    status: StatusEnum
    priority: int
    scheduled_at: Optional[datetime.datetime]
    started_at: Optional[datetime.datetime]
    completed_at: Optional[datetime.datetime]
    cancelled_at: Optional[datetime.datetime]
    cancelled_by: Optional[str]
    cancellation_reason: Optional[str]
    error_message: Optional[str]
    retry_count: int
    max_retries: int
    metadata: Optional[Any]
    variables: Optional[Any]
    progress: Optional[Any]
    created_at: datetime.datetime
    updated_at: datetime.datetime


class CrawlLog(pydantic.BaseModel):
    """Stores detailed crawl execution logs (partitioned by month)"""
    id: int
    job_id: uuid.UUID
    website_id: uuid.UUID
    step_name: Optional[str]
    log_level: LogLevelEnum
    message: str
    context: Optional[Any]
    trace_id: Optional[uuid.UUID]
    created_at: datetime.datetime


class CrawlLog202508(pydantic.BaseModel):
    id: int
    job_id: uuid.UUID
    website_id: uuid.UUID
    step_name: Optional[str]
    log_level: LogLevelEnum
    message: str
    context: Optional[Any]
    trace_id: Optional[uuid.UUID]
    created_at: datetime.datetime


class CrawlLog202509(pydantic.BaseModel):
    id: int
    job_id: uuid.UUID
    website_id: uuid.UUID
    step_name: Optional[str]
    log_level: LogLevelEnum
    message: str
    context: Optional[Any]
    trace_id: Optional[uuid.UUID]
    created_at: datetime.datetime


class CrawlLog202510(pydantic.BaseModel):
    id: int
    job_id: uuid.UUID
    website_id: uuid.UUID
    step_name: Optional[str]
    log_level: LogLevelEnum
    message: str
    context: Optional[Any]
    trace_id: Optional[uuid.UUID]
    created_at: datetime.datetime


class CrawlLog202511(pydantic.BaseModel):
    id: int
    job_id: uuid.UUID
    website_id: uuid.UUID
    step_name: Optional[str]
    log_level: LogLevelEnum
    message: str
    context: Optional[Any]
    trace_id: Optional[uuid.UUID]
    created_at: datetime.datetime


class CrawlLog202512(pydantic.BaseModel):
    id: int
    job_id: uuid.UUID
    website_id: uuid.UUID
    step_name: Optional[str]
    log_level: LogLevelEnum
    message: str
    context: Optional[Any]
    trace_id: Optional[uuid.UUID]
    created_at: datetime.datetime


class CrawlLog202601(pydantic.BaseModel):
    id: int
    job_id: uuid.UUID
    website_id: uuid.UUID
    step_name: Optional[str]
    log_level: LogLevelEnum
    message: str
    context: Optional[Any]
    trace_id: Optional[uuid.UUID]
    created_at: datetime.datetime


class CrawlLog202602(pydantic.BaseModel):
    id: int
    job_id: uuid.UUID
    website_id: uuid.UUID
    step_name: Optional[str]
    log_level: LogLevelEnum
    message: str
    context: Optional[Any]
    trace_id: Optional[uuid.UUID]
    created_at: datetime.datetime


class CrawledPage(pydantic.BaseModel):
    """Stores crawled page data and content"""
    id: uuid.UUID
    website_id: uuid.UUID
    job_id: uuid.UUID
    url: str
    url_hash: str
    content_hash: str
    title: Optional[str]
    extracted_content: Optional[str]
    metadata: Optional[Any]
    gcs_html_path: Optional[str]
    gcs_documents: Optional[Any]
    is_duplicate: bool
    duplicate_of: Optional[uuid.UUID]
    similarity_score: Optional[int]
    crawled_at: datetime.datetime
    created_at: datetime.datetime


class DeadLetterQueue(pydantic.BaseModel):
    id: int
    job_id: uuid.UUID
    seed_url: str
    website_id: Optional[uuid.UUID]
    job_type: JobTypeEnum
    priority: int
    error_category: ErrorCategoryEnum
    error_message: str
    stack_trace: Optional[str]
    http_status: Optional[int]
    total_attempts: int
    first_attempt_at: datetime.datetime
    last_attempt_at: datetime.datetime
    added_to_dlq_at: datetime.datetime
    retry_attempted: bool
    retry_attempted_at: Optional[datetime.datetime]
    retry_success: Optional[bool]
    resolved_at: Optional[datetime.datetime]
    resolution_notes: Optional[str]


class DuplicateGroup(pydantic.BaseModel):
    id: uuid.UUID
    canonical_page_id: uuid.UUID
    group_size: int
    created_at: datetime.datetime
    updated_at: datetime.datetime


class DuplicateRelationship(pydantic.BaseModel):
    id: int
    group_id: uuid.UUID
    duplicate_page_id: uuid.UUID
    detection_method: str
    similarity_score: Optional[int]
    confidence_threshold: Optional[int]
    detected_at: datetime.datetime
    detected_by: Optional[str]


class RetryHistory(pydantic.BaseModel):
    id: int
    job_id: uuid.UUID
    attempt_number: int
    error_category: ErrorCategoryEnum
    error_message: str
    stack_trace: Optional[str]
    retry_delay_seconds: int
    attempted_at: datetime.datetime


class RetryPolicy(pydantic.BaseModel):
    id: uuid.UUID
    error_category: ErrorCategoryEnum
    is_retryable: bool
    max_attempts: int
    backoff_strategy: BackoffStrategyEnum
    initial_delay_seconds: int
    max_delay_seconds: int
    backoff_multiplier: float
    description: Optional[str]
    created_at: datetime.datetime
    updated_at: datetime.datetime


class ScheduledJob(pydantic.BaseModel):
    """Stores scheduled crawl job configurations with cron schedules"""
    id: uuid.UUID
    website_id: uuid.UUID
    # Cron expression defining when the job should run
    cron_schedule: str
    # Next scheduled execution time
    next_run_time: datetime.datetime
    # Most recent execution time
    last_run_time: Optional[datetime.datetime]
    # Flag to pause/resume schedule without deleting
    is_active: bool
    # Job-specific configuration overrides
    job_config: Optional[Any]
    created_at: datetime.datetime
    updated_at: datetime.datetime
    # IANA timezone name (e.g., UTC, America/New_York, Asia/Jakarta) for schedule calculations. Validated at application layer.
    timezone: str


class Website(pydantic.BaseModel):
    """Stores website configurations and metadata"""
    id: uuid.UUID
    name: str
    base_url: str
    config: Any
    status: StatusEnum
    created_at: datetime.datetime
    updated_at: datetime.datetime
    created_by: Optional[str]
    # Default cron schedule expression for this website (default: "0 0 1,15 * *" runs on 1st and 15th at midnight, approximately every 2 weeks)
    cron_schedule: Optional[str]
    # Timestamp when website was soft deleted (NULL = active)
    deleted_at: Optional[datetime.datetime]


class WebsiteConfigHistory(pydantic.BaseModel):
    """Stores configuration history for websites to track changes over time"""
    id: uuid.UUID
    website_id: uuid.UUID
    # Version number, incremented with each change (starts at 1)
    version: int
    # Full configuration snapshot at this version
    config: Any
    # User or system that made the change
    changed_by: Optional[str]
    # Optional description of why the change was made
    change_reason: Optional[str]
    created_at: datetime.datetime
