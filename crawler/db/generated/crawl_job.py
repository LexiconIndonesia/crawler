# Code generated by sqlc. DO NOT EDIT.
# versions:
#   sqlc v1.30.0
# source: crawl_job.sql
import datetime
from typing import Any, AsyncIterator, Optional
import uuid

import sqlalchemy
import sqlalchemy.ext.asyncio

from crawler.db.generated import models


CANCEL_CRAWL_JOB = """-- name: cancel_crawl_job \\:one
UPDATE crawl_job
SET
    status = 'cancelled',
    cancelled_at = CURRENT_TIMESTAMP,
    cancelled_by = :p1,
    cancellation_reason = :p2,
    updated_at = CURRENT_TIMESTAMP
WHERE id = :p3
RETURNING id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at
"""


COUNT_CRAWL_JOBS = """-- name: count_crawl_jobs \\:one
SELECT COUNT(*) FROM crawl_job
WHERE
    website_id = COALESCE(:p1, website_id)
    AND status = COALESCE(:p2, status)
"""


CREATE_CRAWL_JOB = """-- name: create_crawl_job \\:one
INSERT INTO crawl_job (
    website_id,
    job_type,
    seed_url,
    inline_config,
    priority,
    scheduled_at,
    max_retries,
    metadata,
    variables
) VALUES (
    :p1,
    COALESCE(:p2, 'one_time'\\:\\:job_type_enum),
    :p3,
    :p4,
    COALESCE(:p5, 5),
    :p6,
    COALESCE(:p7, 3),
    :p8,
    :p9
)
RETURNING id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at
"""


CREATE_SEED_URL_SUBMISSION = """-- name: create_seed_url_submission \\:one
INSERT INTO crawl_job (
    seed_url,
    inline_config,
    variables,
    job_type,
    priority,
    scheduled_at,
    max_retries,
    metadata
) VALUES (
    :p1,
    :p2,
    :p3,
    COALESCE(:p4, 'one_time'\\:\\:job_type_enum),
    COALESCE(:p5, 5),
    :p6,
    COALESCE(:p7, 3),
    :p8
)
RETURNING id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at
"""


CREATE_TEMPLATE_BASED_JOB = """-- name: create_template_based_job \\:one
INSERT INTO crawl_job (
    website_id,
    seed_url,
    variables,
    job_type,
    priority,
    scheduled_at,
    max_retries,
    metadata
) VALUES (
    :p1,
    :p2,
    :p3,
    COALESCE(:p4, 'one_time'\\:\\:job_type_enum),
    COALESCE(:p5, 5),
    :p6,
    COALESCE(:p7, 3),
    :p8
)
RETURNING id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at
"""


DELETE_OLD_COMPLETED_JOBS = """-- name: delete_old_completed_jobs \\:exec
DELETE FROM crawl_job
WHERE status IN ('completed', 'cancelled')
    AND completed_at < CURRENT_TIMESTAMP - INTERVAL '30 days'
"""


GET_CRAWL_JOB_BY_ID = """-- name: get_crawl_job_by_id \\:one
SELECT id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at FROM crawl_job
WHERE id = :p1
"""


GET_FAILED_JOBS_FOR_RETRY = """-- name: get_failed_jobs_for_retry \\:many
SELECT id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at FROM crawl_job
WHERE status = 'failed'
    AND retry_count < max_retries
ORDER BY priority DESC, created_at ASC
LIMIT :p1
"""


GET_INLINE_CONFIG_JOBS = """-- name: get_inline_config_jobs \\:many
SELECT id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at FROM crawl_job
WHERE website_id IS NULL
    AND inline_config IS NOT NULL
ORDER BY created_at DESC
LIMIT :p2 OFFSET :p1
"""


GET_JOBS_BY_SEED_URL = """-- name: get_jobs_by_seed_url \\:many
SELECT id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at FROM crawl_job
WHERE seed_url = :p1
ORDER BY created_at DESC
LIMIT :p3 OFFSET :p2
"""


GET_JOBS_BY_WEBSITE = """-- name: get_jobs_by_website \\:many
SELECT id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at FROM crawl_job
WHERE website_id = :p1
ORDER BY created_at DESC
LIMIT :p3 OFFSET :p2
"""


GET_PENDING_JOBS = """-- name: get_pending_jobs \\:many
SELECT id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at FROM crawl_job
WHERE status = 'pending'
    AND (scheduled_at IS NULL OR scheduled_at <= CURRENT_TIMESTAMP)
ORDER BY priority DESC, created_at ASC
LIMIT :p1
"""


GET_RUNNING_JOBS = """-- name: get_running_jobs \\:many
SELECT id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at FROM crawl_job
WHERE status = 'running'
ORDER BY started_at ASC
"""


INCREMENT_JOB_RETRY_COUNT = """-- name: increment_job_retry_count \\:one
UPDATE crawl_job
SET
    retry_count = retry_count + 1,
    updated_at = CURRENT_TIMESTAMP
WHERE id = :p1
RETURNING id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at
"""


LIST_CRAWL_JOBS = """-- name: list_crawl_jobs \\:many
SELECT id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at FROM crawl_job
WHERE
    website_id = COALESCE(:p1, website_id)
    AND status = COALESCE(:p2, status)
    AND job_type = COALESCE(:p3, job_type)
ORDER BY priority DESC, created_at ASC
LIMIT :p5 OFFSET :p4
"""


UPDATE_CRAWL_JOB_PROGRESS = """-- name: update_crawl_job_progress \\:one
UPDATE crawl_job
SET
    progress = :p1,
    updated_at = CURRENT_TIMESTAMP
WHERE id = :p2
RETURNING id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at
"""


UPDATE_CRAWL_JOB_STATUS = """-- name: update_crawl_job_status \\:one
UPDATE crawl_job
SET
    status = :p1\\:\\:status_enum,
    started_at = CASE WHEN :p1\\:\\:status_enum = 'running'\\:\\:status_enum THEN COALESCE(:p2, CURRENT_TIMESTAMP) ELSE started_at END,
    completed_at = CASE WHEN :p1\\:\\:status_enum IN ('completed'\\:\\:status_enum, 'failed'\\:\\:status_enum, 'cancelled'\\:\\:status_enum) THEN COALESCE(:p3, CURRENT_TIMESTAMP) ELSE completed_at END,
    error_message = :p4,
    updated_at = CURRENT_TIMESTAMP
WHERE id = :p5
RETURNING id, website_id, job_type, seed_url, inline_config, status, priority, scheduled_at, started_at, completed_at, cancelled_at, cancelled_by, cancellation_reason, error_message, retry_count, max_retries, metadata, variables, progress, created_at, updated_at
"""


class AsyncQuerier:
    def __init__(self, conn: sqlalchemy.ext.asyncio.AsyncConnection):
        self._conn = conn

    async def cancel_crawl_job(self, *, cancelled_by: Optional[str], cancellation_reason: Optional[str], id: uuid.UUID) -> Optional[models.CrawlJob]:
        row = (await self._conn.execute(sqlalchemy.text(CANCEL_CRAWL_JOB), {"p1": cancelled_by, "p2": cancellation_reason, "p3": id})).first()
        if row is None:
            return None
        return models.CrawlJob(
            id=row[0],
            website_id=row[1],
            job_type=row[2],
            seed_url=row[3],
            inline_config=row[4],
            status=row[5],
            priority=row[6],
            scheduled_at=row[7],
            started_at=row[8],
            completed_at=row[9],
            cancelled_at=row[10],
            cancelled_by=row[11],
            cancellation_reason=row[12],
            error_message=row[13],
            retry_count=row[14],
            max_retries=row[15],
            metadata=row[16],
            variables=row[17],
            progress=row[18],
            created_at=row[19],
            updated_at=row[20],
        )

    async def count_crawl_jobs(self, *, website_id: Optional[uuid.UUID], status: models.StatusEnum) -> Optional[int]:
        row = (await self._conn.execute(sqlalchemy.text(COUNT_CRAWL_JOBS), {"p1": website_id, "p2": status})).first()
        if row is None:
            return None
        return row[0]

    async def create_crawl_job(self, *, website_id: Optional[uuid.UUID], job_type: Optional[Any], seed_url: str, inline_config: Optional[Any], priority: Optional[Any], scheduled_at: Optional[datetime.datetime], max_retries: Optional[Any], metadata: Optional[Any], variables: Optional[Any]) -> Optional[models.CrawlJob]:
        row = (await self._conn.execute(sqlalchemy.text(CREATE_CRAWL_JOB), {
            "p1": website_id,
            "p2": job_type,
            "p3": seed_url,
            "p4": inline_config,
            "p5": priority,
            "p6": scheduled_at,
            "p7": max_retries,
            "p8": metadata,
            "p9": variables,
        })).first()
        if row is None:
            return None
        return models.CrawlJob(
            id=row[0],
            website_id=row[1],
            job_type=row[2],
            seed_url=row[3],
            inline_config=row[4],
            status=row[5],
            priority=row[6],
            scheduled_at=row[7],
            started_at=row[8],
            completed_at=row[9],
            cancelled_at=row[10],
            cancelled_by=row[11],
            cancellation_reason=row[12],
            error_message=row[13],
            retry_count=row[14],
            max_retries=row[15],
            metadata=row[16],
            variables=row[17],
            progress=row[18],
            created_at=row[19],
            updated_at=row[20],
        )

    async def create_seed_url_submission(self, *, seed_url: str, inline_config: Optional[Any], variables: Optional[Any], job_type: Optional[Any], priority: Optional[Any], scheduled_at: Optional[datetime.datetime], max_retries: Optional[Any], metadata: Optional[Any]) -> Optional[models.CrawlJob]:
        row = (await self._conn.execute(sqlalchemy.text(CREATE_SEED_URL_SUBMISSION), {
            "p1": seed_url,
            "p2": inline_config,
            "p3": variables,
            "p4": job_type,
            "p5": priority,
            "p6": scheduled_at,
            "p7": max_retries,
            "p8": metadata,
        })).first()
        if row is None:
            return None
        return models.CrawlJob(
            id=row[0],
            website_id=row[1],
            job_type=row[2],
            seed_url=row[3],
            inline_config=row[4],
            status=row[5],
            priority=row[6],
            scheduled_at=row[7],
            started_at=row[8],
            completed_at=row[9],
            cancelled_at=row[10],
            cancelled_by=row[11],
            cancellation_reason=row[12],
            error_message=row[13],
            retry_count=row[14],
            max_retries=row[15],
            metadata=row[16],
            variables=row[17],
            progress=row[18],
            created_at=row[19],
            updated_at=row[20],
        )

    async def create_template_based_job(self, *, website_id: Optional[uuid.UUID], seed_url: str, variables: Optional[Any], job_type: Optional[Any], priority: Optional[Any], scheduled_at: Optional[datetime.datetime], max_retries: Optional[Any], metadata: Optional[Any]) -> Optional[models.CrawlJob]:
        row = (await self._conn.execute(sqlalchemy.text(CREATE_TEMPLATE_BASED_JOB), {
            "p1": website_id,
            "p2": seed_url,
            "p3": variables,
            "p4": job_type,
            "p5": priority,
            "p6": scheduled_at,
            "p7": max_retries,
            "p8": metadata,
        })).first()
        if row is None:
            return None
        return models.CrawlJob(
            id=row[0],
            website_id=row[1],
            job_type=row[2],
            seed_url=row[3],
            inline_config=row[4],
            status=row[5],
            priority=row[6],
            scheduled_at=row[7],
            started_at=row[8],
            completed_at=row[9],
            cancelled_at=row[10],
            cancelled_by=row[11],
            cancellation_reason=row[12],
            error_message=row[13],
            retry_count=row[14],
            max_retries=row[15],
            metadata=row[16],
            variables=row[17],
            progress=row[18],
            created_at=row[19],
            updated_at=row[20],
        )

    async def delete_old_completed_jobs(self) -> None:
        await self._conn.execute(sqlalchemy.text(DELETE_OLD_COMPLETED_JOBS))

    async def get_crawl_job_by_id(self, *, id: uuid.UUID) -> Optional[models.CrawlJob]:
        row = (await self._conn.execute(sqlalchemy.text(GET_CRAWL_JOB_BY_ID), {"p1": id})).first()
        if row is None:
            return None
        return models.CrawlJob(
            id=row[0],
            website_id=row[1],
            job_type=row[2],
            seed_url=row[3],
            inline_config=row[4],
            status=row[5],
            priority=row[6],
            scheduled_at=row[7],
            started_at=row[8],
            completed_at=row[9],
            cancelled_at=row[10],
            cancelled_by=row[11],
            cancellation_reason=row[12],
            error_message=row[13],
            retry_count=row[14],
            max_retries=row[15],
            metadata=row[16],
            variables=row[17],
            progress=row[18],
            created_at=row[19],
            updated_at=row[20],
        )

    async def get_failed_jobs_for_retry(self, *, limit_count: int) -> AsyncIterator[models.CrawlJob]:
        result = await self._conn.stream(sqlalchemy.text(GET_FAILED_JOBS_FOR_RETRY), {"p1": limit_count})
        async for row in result:
            yield models.CrawlJob(
                id=row[0],
                website_id=row[1],
                job_type=row[2],
                seed_url=row[3],
                inline_config=row[4],
                status=row[5],
                priority=row[6],
                scheduled_at=row[7],
                started_at=row[8],
                completed_at=row[9],
                cancelled_at=row[10],
                cancelled_by=row[11],
                cancellation_reason=row[12],
                error_message=row[13],
                retry_count=row[14],
                max_retries=row[15],
                metadata=row[16],
                variables=row[17],
                progress=row[18],
                created_at=row[19],
                updated_at=row[20],
            )

    async def get_inline_config_jobs(self, *, offset_count: int, limit_count: int) -> AsyncIterator[models.CrawlJob]:
        result = await self._conn.stream(sqlalchemy.text(GET_INLINE_CONFIG_JOBS), {"p1": offset_count, "p2": limit_count})
        async for row in result:
            yield models.CrawlJob(
                id=row[0],
                website_id=row[1],
                job_type=row[2],
                seed_url=row[3],
                inline_config=row[4],
                status=row[5],
                priority=row[6],
                scheduled_at=row[7],
                started_at=row[8],
                completed_at=row[9],
                cancelled_at=row[10],
                cancelled_by=row[11],
                cancellation_reason=row[12],
                error_message=row[13],
                retry_count=row[14],
                max_retries=row[15],
                metadata=row[16],
                variables=row[17],
                progress=row[18],
                created_at=row[19],
                updated_at=row[20],
            )

    async def get_jobs_by_seed_url(self, *, seed_url: str, offset_count: int, limit_count: int) -> AsyncIterator[models.CrawlJob]:
        result = await self._conn.stream(sqlalchemy.text(GET_JOBS_BY_SEED_URL), {"p1": seed_url, "p2": offset_count, "p3": limit_count})
        async for row in result:
            yield models.CrawlJob(
                id=row[0],
                website_id=row[1],
                job_type=row[2],
                seed_url=row[3],
                inline_config=row[4],
                status=row[5],
                priority=row[6],
                scheduled_at=row[7],
                started_at=row[8],
                completed_at=row[9],
                cancelled_at=row[10],
                cancelled_by=row[11],
                cancellation_reason=row[12],
                error_message=row[13],
                retry_count=row[14],
                max_retries=row[15],
                metadata=row[16],
                variables=row[17],
                progress=row[18],
                created_at=row[19],
                updated_at=row[20],
            )

    async def get_jobs_by_website(self, *, website_id: Optional[uuid.UUID], offset_count: int, limit_count: int) -> AsyncIterator[models.CrawlJob]:
        result = await self._conn.stream(sqlalchemy.text(GET_JOBS_BY_WEBSITE), {"p1": website_id, "p2": offset_count, "p3": limit_count})
        async for row in result:
            yield models.CrawlJob(
                id=row[0],
                website_id=row[1],
                job_type=row[2],
                seed_url=row[3],
                inline_config=row[4],
                status=row[5],
                priority=row[6],
                scheduled_at=row[7],
                started_at=row[8],
                completed_at=row[9],
                cancelled_at=row[10],
                cancelled_by=row[11],
                cancellation_reason=row[12],
                error_message=row[13],
                retry_count=row[14],
                max_retries=row[15],
                metadata=row[16],
                variables=row[17],
                progress=row[18],
                created_at=row[19],
                updated_at=row[20],
            )

    async def get_pending_jobs(self, *, limit_count: int) -> AsyncIterator[models.CrawlJob]:
        result = await self._conn.stream(sqlalchemy.text(GET_PENDING_JOBS), {"p1": limit_count})
        async for row in result:
            yield models.CrawlJob(
                id=row[0],
                website_id=row[1],
                job_type=row[2],
                seed_url=row[3],
                inline_config=row[4],
                status=row[5],
                priority=row[6],
                scheduled_at=row[7],
                started_at=row[8],
                completed_at=row[9],
                cancelled_at=row[10],
                cancelled_by=row[11],
                cancellation_reason=row[12],
                error_message=row[13],
                retry_count=row[14],
                max_retries=row[15],
                metadata=row[16],
                variables=row[17],
                progress=row[18],
                created_at=row[19],
                updated_at=row[20],
            )

    async def get_running_jobs(self) -> AsyncIterator[models.CrawlJob]:
        result = await self._conn.stream(sqlalchemy.text(GET_RUNNING_JOBS))
        async for row in result:
            yield models.CrawlJob(
                id=row[0],
                website_id=row[1],
                job_type=row[2],
                seed_url=row[3],
                inline_config=row[4],
                status=row[5],
                priority=row[6],
                scheduled_at=row[7],
                started_at=row[8],
                completed_at=row[9],
                cancelled_at=row[10],
                cancelled_by=row[11],
                cancellation_reason=row[12],
                error_message=row[13],
                retry_count=row[14],
                max_retries=row[15],
                metadata=row[16],
                variables=row[17],
                progress=row[18],
                created_at=row[19],
                updated_at=row[20],
            )

    async def increment_job_retry_count(self, *, id: uuid.UUID) -> Optional[models.CrawlJob]:
        row = (await self._conn.execute(sqlalchemy.text(INCREMENT_JOB_RETRY_COUNT), {"p1": id})).first()
        if row is None:
            return None
        return models.CrawlJob(
            id=row[0],
            website_id=row[1],
            job_type=row[2],
            seed_url=row[3],
            inline_config=row[4],
            status=row[5],
            priority=row[6],
            scheduled_at=row[7],
            started_at=row[8],
            completed_at=row[9],
            cancelled_at=row[10],
            cancelled_by=row[11],
            cancellation_reason=row[12],
            error_message=row[13],
            retry_count=row[14],
            max_retries=row[15],
            metadata=row[16],
            variables=row[17],
            progress=row[18],
            created_at=row[19],
            updated_at=row[20],
        )

    async def list_crawl_jobs(self, *, website_id: Optional[uuid.UUID], status: models.StatusEnum, job_type: models.JobTypeEnum, offset_count: int, limit_count: int) -> AsyncIterator[models.CrawlJob]:
        result = await self._conn.stream(sqlalchemy.text(LIST_CRAWL_JOBS), {
            "p1": website_id,
            "p2": status,
            "p3": job_type,
            "p4": offset_count,
            "p5": limit_count,
        })
        async for row in result:
            yield models.CrawlJob(
                id=row[0],
                website_id=row[1],
                job_type=row[2],
                seed_url=row[3],
                inline_config=row[4],
                status=row[5],
                priority=row[6],
                scheduled_at=row[7],
                started_at=row[8],
                completed_at=row[9],
                cancelled_at=row[10],
                cancelled_by=row[11],
                cancellation_reason=row[12],
                error_message=row[13],
                retry_count=row[14],
                max_retries=row[15],
                metadata=row[16],
                variables=row[17],
                progress=row[18],
                created_at=row[19],
                updated_at=row[20],
            )

    async def update_crawl_job_progress(self, *, progress: Optional[Any], id: uuid.UUID) -> Optional[models.CrawlJob]:
        row = (await self._conn.execute(sqlalchemy.text(UPDATE_CRAWL_JOB_PROGRESS), {"p1": progress, "p2": id})).first()
        if row is None:
            return None
        return models.CrawlJob(
            id=row[0],
            website_id=row[1],
            job_type=row[2],
            seed_url=row[3],
            inline_config=row[4],
            status=row[5],
            priority=row[6],
            scheduled_at=row[7],
            started_at=row[8],
            completed_at=row[9],
            cancelled_at=row[10],
            cancelled_by=row[11],
            cancellation_reason=row[12],
            error_message=row[13],
            retry_count=row[14],
            max_retries=row[15],
            metadata=row[16],
            variables=row[17],
            progress=row[18],
            created_at=row[19],
            updated_at=row[20],
        )

    async def update_crawl_job_status(self, *, status: models.StatusEnum, started_at: Optional[datetime.datetime], completed_at: Optional[datetime.datetime], error_message: Optional[str], id: uuid.UUID) -> Optional[models.CrawlJob]:
        row = (await self._conn.execute(sqlalchemy.text(UPDATE_CRAWL_JOB_STATUS), {
            "p1": status,
            "p2": started_at,
            "p3": completed_at,
            "p4": error_message,
            "p5": id,
        })).first()
        if row is None:
            return None
        return models.CrawlJob(
            id=row[0],
            website_id=row[1],
            job_type=row[2],
            seed_url=row[3],
            inline_config=row[4],
            status=row[5],
            priority=row[6],
            scheduled_at=row[7],
            started_at=row[8],
            completed_at=row[9],
            cancelled_at=row[10],
            cancelled_by=row[11],
            cancellation_reason=row[12],
            error_message=row[13],
            retry_count=row[14],
            max_retries=row[15],
            metadata=row[16],
            variables=row[17],
            progress=row[18],
            created_at=row[19],
            updated_at=row[20],
        )
