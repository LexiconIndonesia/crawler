# Code generated by sqlc. DO NOT EDIT.
# versions:
#   sqlc v1.30.0
# source: crawl_log.sql
import datetime
import pydantic
from typing import Any, AsyncIterator, Optional
import uuid

import sqlalchemy
import sqlalchemy.ext.asyncio

from crawler.db.generated import models


COUNT_JOB_LOGS_FILTERED = """-- name: count_job_logs_filtered \\:one
SELECT COUNT(*) FROM crawl_log
WHERE job_id = :p1
    AND log_level = COALESCE(:p2, log_level)
    AND (:p3\\:\\:TIMESTAMP WITH TIME ZONE IS NULL OR created_at >= :p3\\:\\:TIMESTAMP WITH TIME ZONE)
    AND (:p4\\:\\:TIMESTAMP WITH TIME ZONE IS NULL OR created_at <= :p4\\:\\:TIMESTAMP WITH TIME ZONE)
    AND (:p5\\:\\:TEXT IS NULL OR message ILIKE '%' || :p5\\:\\:TEXT || '%')
"""


COUNT_LOGS_BY_JOB = """-- name: count_logs_by_job \\:one
SELECT COUNT(*) FROM crawl_log
WHERE job_id = :p1
    AND log_level = COALESCE(:p2, log_level)
"""


COUNT_LOGS_BY_WEBSITE = """-- name: count_logs_by_website \\:one
SELECT COUNT(*) FROM crawl_log
WHERE website_id = :p1
    AND log_level = COALESCE(:p2, log_level)
"""


CREATE_CRAWL_LOG = """-- name: create_crawl_log \\:one
INSERT INTO crawl_log (
    job_id,
    website_id,
    step_name,
    log_level,
    message,
    context,
    trace_id
) VALUES (
    :p1,
    :p2,
    :p3,
    :p4,
    :p5,
    :p6,
    :p7
)
RETURNING id, job_id, website_id, step_name, log_level, message, context, trace_id, created_at
"""


DELETE_LOGS_BY_JOB = """-- name: delete_logs_by_job \\:exec
DELETE FROM crawl_log
WHERE job_id = :p1
"""


DELETE_OLD_LOGS = """-- name: delete_old_logs \\:exec
DELETE FROM crawl_log
WHERE created_at < CURRENT_TIMESTAMP - INTERVAL '30 days'
"""


GET_CRAWL_LOG_BY_ID = """-- name: get_crawl_log_by_id \\:one
SELECT id, job_id, website_id, step_name, log_level, message, context, trace_id, created_at FROM crawl_log
WHERE id = :p1
"""


GET_ERROR_LOGS = """-- name: get_error_logs \\:many
SELECT id, job_id, website_id, step_name, log_level, message, context, trace_id, created_at FROM crawl_log
WHERE job_id = :p1
    AND log_level IN ('ERROR', 'CRITICAL')
ORDER BY created_at DESC
LIMIT :p2
"""


GET_JOB_LOGS_FILTERED = """-- name: get_job_logs_filtered \\:many
SELECT id, job_id, website_id, step_name, log_level, message, context, trace_id, created_at FROM crawl_log
WHERE job_id = :p1
    AND log_level = COALESCE(:p2, log_level)
    AND (:p3\\:\\:TIMESTAMP WITH TIME ZONE IS NULL OR created_at >= :p3\\:\\:TIMESTAMP WITH TIME ZONE)
    AND (:p4\\:\\:TIMESTAMP WITH TIME ZONE IS NULL OR created_at <= :p4\\:\\:TIMESTAMP WITH TIME ZONE)
    AND (:p5\\:\\:TEXT IS NULL OR message ILIKE '%' || :p5\\:\\:TEXT || '%')
ORDER BY created_at ASC
OFFSET :p6 LIMIT :p7
"""


GET_LOG_STATS_BY_JOB = """-- name: get_log_stats_by_job \\:one
SELECT
    COUNT(*) as total_logs,
    COUNT(*) FILTER (WHERE log_level = 'DEBUG') as debug_count,
    COUNT(*) FILTER (WHERE log_level = 'INFO') as info_count,
    COUNT(*) FILTER (WHERE log_level = 'WARNING') as warning_count,
    COUNT(*) FILTER (WHERE log_level = 'ERROR') as error_count,
    COUNT(*) FILTER (WHERE log_level = 'CRITICAL') as critical_count
FROM crawl_log
WHERE job_id = :p1
"""


class GetLogStatsByJobRow(pydantic.BaseModel):
    total_logs: int
    debug_count: int
    info_count: int
    warning_count: int
    error_count: int
    critical_count: int


GET_LOGS_BY_TIME_RANGE = """-- name: get_logs_by_time_range \\:many
SELECT id, job_id, website_id, step_name, log_level, message, context, trace_id, created_at FROM crawl_log
WHERE job_id = :p1
    AND created_at >= :p2\\:\\:TIMESTAMP WITH TIME ZONE
    AND created_at <= :p3\\:\\:TIMESTAMP WITH TIME ZONE
    AND log_level = COALESCE(:p4, log_level)
ORDER BY created_at DESC
OFFSET :p5 LIMIT :p6
"""


LIST_LOGS_BY_JOB = """-- name: list_logs_by_job \\:many
SELECT id, job_id, website_id, step_name, log_level, message, context, trace_id, created_at FROM crawl_log
WHERE job_id = :p1
    AND log_level = COALESCE(:p2, log_level)
ORDER BY created_at DESC
OFFSET :p3 LIMIT :p4
"""


LIST_LOGS_BY_TRACE_ID = """-- name: list_logs_by_trace_id \\:many
SELECT id, job_id, website_id, step_name, log_level, message, context, trace_id, created_at FROM crawl_log
WHERE trace_id = :p1
ORDER BY created_at ASC
"""


LIST_LOGS_BY_WEBSITE = """-- name: list_logs_by_website \\:many
SELECT id, job_id, website_id, step_name, log_level, message, context, trace_id, created_at FROM crawl_log
WHERE website_id = :p1
    AND log_level = COALESCE(:p2, log_level)
ORDER BY created_at DESC
OFFSET :p3 LIMIT :p4
"""


STREAM_LOGS_BY_JOB = """-- name: stream_logs_by_job \\:many
SELECT id, job_id, website_id, step_name, log_level, message, context, trace_id, created_at FROM crawl_log
WHERE job_id = :p1
    AND created_at > :p2\\:\\:TIMESTAMP WITH TIME ZONE
    AND log_level = COALESCE(:p3, log_level)
ORDER BY created_at ASC
LIMIT :p4
"""


class AsyncQuerier:
    def __init__(self, conn: sqlalchemy.ext.asyncio.AsyncConnection):
        self._conn = conn

    async def count_job_logs_filtered(self, *, job_id: uuid.UUID, log_level: models.LogLevelEnum, start_time: datetime.datetime, end_time: datetime.datetime, search_text: str) -> Optional[int]:
        row = (await self._conn.execute(sqlalchemy.text(COUNT_JOB_LOGS_FILTERED), {
            "p1": job_id,
            "p2": log_level,
            "p3": start_time,
            "p4": end_time,
            "p5": search_text,
        })).first()
        if row is None:
            return None
        return row[0]

    async def count_logs_by_job(self, *, job_id: uuid.UUID, log_level: models.LogLevelEnum) -> Optional[int]:
        row = (await self._conn.execute(sqlalchemy.text(COUNT_LOGS_BY_JOB), {"p1": job_id, "p2": log_level})).first()
        if row is None:
            return None
        return row[0]

    async def count_logs_by_website(self, *, website_id: uuid.UUID, log_level: models.LogLevelEnum) -> Optional[int]:
        row = (await self._conn.execute(sqlalchemy.text(COUNT_LOGS_BY_WEBSITE), {"p1": website_id, "p2": log_level})).first()
        if row is None:
            return None
        return row[0]

    async def create_crawl_log(self, *, job_id: uuid.UUID, website_id: uuid.UUID, step_name: Optional[str], log_level: models.LogLevelEnum, message: str, context: Optional[Any], trace_id: Optional[uuid.UUID]) -> Optional[models.CrawlLog]:
        row = (await self._conn.execute(sqlalchemy.text(CREATE_CRAWL_LOG), {
            "p1": job_id,
            "p2": website_id,
            "p3": step_name,
            "p4": log_level,
            "p5": message,
            "p6": context,
            "p7": trace_id,
        })).first()
        if row is None:
            return None
        return models.CrawlLog(
            id=row[0],
            job_id=row[1],
            website_id=row[2],
            step_name=row[3],
            log_level=row[4],
            message=row[5],
            context=row[6],
            trace_id=row[7],
            created_at=row[8],
        )

    async def delete_logs_by_job(self, *, job_id: uuid.UUID) -> None:
        await self._conn.execute(sqlalchemy.text(DELETE_LOGS_BY_JOB), {"p1": job_id})

    async def delete_old_logs(self) -> None:
        await self._conn.execute(sqlalchemy.text(DELETE_OLD_LOGS))

    async def get_crawl_log_by_id(self, *, id: int) -> Optional[models.CrawlLog]:
        row = (await self._conn.execute(sqlalchemy.text(GET_CRAWL_LOG_BY_ID), {"p1": id})).first()
        if row is None:
            return None
        return models.CrawlLog(
            id=row[0],
            job_id=row[1],
            website_id=row[2],
            step_name=row[3],
            log_level=row[4],
            message=row[5],
            context=row[6],
            trace_id=row[7],
            created_at=row[8],
        )

    async def get_error_logs(self, *, job_id: uuid.UUID, limit_count: int) -> AsyncIterator[models.CrawlLog]:
        result = await self._conn.stream(sqlalchemy.text(GET_ERROR_LOGS), {"p1": job_id, "p2": limit_count})
        async for row in result:
            yield models.CrawlLog(
                id=row[0],
                job_id=row[1],
                website_id=row[2],
                step_name=row[3],
                log_level=row[4],
                message=row[5],
                context=row[6],
                trace_id=row[7],
                created_at=row[8],
            )

    async def get_job_logs_filtered(self, *, job_id: uuid.UUID, log_level: models.LogLevelEnum, start_time: datetime.datetime, end_time: datetime.datetime, search_text: str, offset_count: int, limit_count: int) -> AsyncIterator[models.CrawlLog]:
        result = await self._conn.stream(sqlalchemy.text(GET_JOB_LOGS_FILTERED), {
            "p1": job_id,
            "p2": log_level,
            "p3": start_time,
            "p4": end_time,
            "p5": search_text,
            "p6": offset_count,
            "p7": limit_count,
        })
        async for row in result:
            yield models.CrawlLog(
                id=row[0],
                job_id=row[1],
                website_id=row[2],
                step_name=row[3],
                log_level=row[4],
                message=row[5],
                context=row[6],
                trace_id=row[7],
                created_at=row[8],
            )

    async def get_log_stats_by_job(self, *, job_id: uuid.UUID) -> Optional[GetLogStatsByJobRow]:
        row = (await self._conn.execute(sqlalchemy.text(GET_LOG_STATS_BY_JOB), {"p1": job_id})).first()
        if row is None:
            return None
        return GetLogStatsByJobRow(
            total_logs=row[0],
            debug_count=row[1],
            info_count=row[2],
            warning_count=row[3],
            error_count=row[4],
            critical_count=row[5],
        )

    async def get_logs_by_time_range(self, *, job_id: uuid.UUID, start_time: datetime.datetime, end_time: datetime.datetime, log_level: models.LogLevelEnum, offset_count: int, limit_count: int) -> AsyncIterator[models.CrawlLog]:
        result = await self._conn.stream(sqlalchemy.text(GET_LOGS_BY_TIME_RANGE), {
            "p1": job_id,
            "p2": start_time,
            "p3": end_time,
            "p4": log_level,
            "p5": offset_count,
            "p6": limit_count,
        })
        async for row in result:
            yield models.CrawlLog(
                id=row[0],
                job_id=row[1],
                website_id=row[2],
                step_name=row[3],
                log_level=row[4],
                message=row[5],
                context=row[6],
                trace_id=row[7],
                created_at=row[8],
            )

    async def list_logs_by_job(self, *, job_id: uuid.UUID, log_level: models.LogLevelEnum, offset_count: int, limit_count: int) -> AsyncIterator[models.CrawlLog]:
        result = await self._conn.stream(sqlalchemy.text(LIST_LOGS_BY_JOB), {
            "p1": job_id,
            "p2": log_level,
            "p3": offset_count,
            "p4": limit_count,
        })
        async for row in result:
            yield models.CrawlLog(
                id=row[0],
                job_id=row[1],
                website_id=row[2],
                step_name=row[3],
                log_level=row[4],
                message=row[5],
                context=row[6],
                trace_id=row[7],
                created_at=row[8],
            )

    async def list_logs_by_trace_id(self, *, trace_id: Optional[uuid.UUID]) -> AsyncIterator[models.CrawlLog]:
        result = await self._conn.stream(sqlalchemy.text(LIST_LOGS_BY_TRACE_ID), {"p1": trace_id})
        async for row in result:
            yield models.CrawlLog(
                id=row[0],
                job_id=row[1],
                website_id=row[2],
                step_name=row[3],
                log_level=row[4],
                message=row[5],
                context=row[6],
                trace_id=row[7],
                created_at=row[8],
            )

    async def list_logs_by_website(self, *, website_id: uuid.UUID, log_level: models.LogLevelEnum, offset_count: int, limit_count: int) -> AsyncIterator[models.CrawlLog]:
        result = await self._conn.stream(sqlalchemy.text(LIST_LOGS_BY_WEBSITE), {
            "p1": website_id,
            "p2": log_level,
            "p3": offset_count,
            "p4": limit_count,
        })
        async for row in result:
            yield models.CrawlLog(
                id=row[0],
                job_id=row[1],
                website_id=row[2],
                step_name=row[3],
                log_level=row[4],
                message=row[5],
                context=row[6],
                trace_id=row[7],
                created_at=row[8],
            )

    async def stream_logs_by_job(self, *, job_id: uuid.UUID, after_timestamp: datetime.datetime, log_level: models.LogLevelEnum, limit_count: int) -> AsyncIterator[models.CrawlLog]:
        result = await self._conn.stream(sqlalchemy.text(STREAM_LOGS_BY_JOB), {
            "p1": job_id,
            "p2": after_timestamp,
            "p3": log_level,
            "p4": limit_count,
        })
        async for row in result:
            yield models.CrawlLog(
                id=row[0],
                job_id=row[1],
                website_id=row[2],
                step_name=row[3],
                log_level=row[4],
                message=row[5],
                context=row[6],
                trace_id=row[7],
                created_at=row[8],
            )
