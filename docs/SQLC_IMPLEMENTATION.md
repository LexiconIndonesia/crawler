# sqlc Implementation Guide

This project uses **sqlc** to generate type-safe Python code from SQL queries. This provides compile-time type safety, better performance than ORMs, and full SQL flexibility.

## Overview

- **SQL Queries**: Plain SQL files in `sql/queries/`
- **Schema**: Database schema in `sql/schema/`
- **Generated Code**: Type-safe Python in `crawler/db/generated/`
- **Repositories**: High-level wrappers in `crawler/db/repositories.py`

## Architecture

```
sql/
├── schema/
│   └── 001_initial_schema.sql    # Database schema definition
└── queries/
    ├── website.sql                # Website queries
    ├── crawl_job.sql              # Job queries
    ├── crawled_page.sql           # Page queries
    ├── content_hash.sql           # Hash queries
    └── crawl_log.sql              # Log queries

crawler/db/
├── generated/                     # Auto-generated by sqlc (DO NOT EDIT)
│   ├── models.py                  # Pydantic models
│   ├── website.py                 # Website queries
│   ├── crawl_job.py               # Job queries
│   ├── crawled_page.py            # Page queries
│   ├── content_hash.py            # Hash queries
│   └── crawl_log.py               # Log queries
└── repositories.py                # High-level repository wrappers
```

## Configuration

sqlc is configured in `sqlc.yaml`:

```yaml
version: "2"
plugins:
  - name: py
    wasm:
      url: https://downloads.sqlc.dev/plugin/sqlc-gen-python_1.3.0.wasm
      sha256: fbedae96b5ecae2380a70fb5b925fd4bff58a6cfb1f3140375d098fbab7b3a3c
sql:
  - schema: "sql/schema"
    queries: "sql/queries"
    engine: "postgresql"
    codegen:
      - plugin: py
        out: crawler/db/generated
        options:
          package: generated
          emit_sync_querier: false
          emit_async_querier: true
          emit_pydantic_models: true
```

## Writing SQL Queries

### Query Annotations

sqlc uses special comments to understand queries:

```sql
-- name: GetWebsiteByID :one
SELECT * FROM website
WHERE id = $1;
```

**Annotations:**
- `:one` - Returns single row (or None)
- `:many` - Returns list of rows
- `:exec` - No return value (INSERT/UPDATE/DELETE)

### Parameter Placeholders

Use `$1, $2, $3...` for positional parameters:

```sql
-- name: CreateWebsite :one
INSERT INTO website (name, base_url, config)
VALUES ($1, $2, $3)
RETURNING *;
```

### NULL Handling with COALESCE

Use `COALESCE` for optional parameters:

```sql
-- name: UpdateWebsite :one
UPDATE website
SET
    name = COALESCE($2, name),
    base_url = COALESCE($3, base_url),
    updated_at = CURRENT_TIMESTAMP
WHERE id = $1
RETURNING *;
```

## Using Generated Code

### Direct Query Usage

```python
from crawler.db.generated import website, models
from sqlalchemy.ext.asyncio import AsyncConnection

async def example(conn: AsyncConnection):
    # Create website
    site = await website.create_website(
        conn,
        name="example",
        base_url="https://example.com",
        config={"max_depth": 3},
        created_by="admin",
        status=None  # Will use default 'active'
    )

    # Get by ID
    fetched = await website.get_website_by_id(conn, id=str(site.id))

    # List with pagination
    sites = []
    async for s in website.list_websites(
        conn, status="active", limit=10, offset=0
    ):
        sites.append(s)
```

### Repository Pattern (Recommended)

```python
from crawler.db import get_db
from crawler.db.repositories import WebsiteRepository

async def example():
    async with get_db() as session:
        async with session.begin():
            # Note: session.connection() returns AsyncConnection directly (no await)
            repo = WebsiteRepository(session.connection())

            # Create
            website = await repo.create(
                name="example",
                base_url="https://example.com",
                config={"max_depth": 3}
            )

            # Get by ID
            fetched = await repo.get_by_id(str(website.id))

            # List
            websites = await repo.list(status="active", limit=10)

            # Update
            updated = await repo.update(
                str(website.id),
                status="inactive"
            )
```

## Generated Models

sqlc generates Pydantic models from your schema:

```python
from crawler.db.generated.models import Website, StatusEnum

# Type-safe model
website = Website(
    id="uuid-here",
    name="example",
    base_url="https://example.com",
    config={"max_depth": 3},
    status=StatusEnum.ACTIVE,
    created_at=datetime.now(),
    updated_at=datetime.now(),
    created_by="admin"
)

# Enum type safety
website.status  # Type: StatusEnum
website.status.value  # "active"
```

## Regenerating Code

After modifying SQL queries, regenerate Python code:

```bash
# Using Makefile
make sqlc-generate

# Or directly
sqlc generate
```

**When to regenerate:**
- After adding new queries to `sql/queries/*.sql`
- After modifying existing queries
- After changing database schema in `sql/schema/*.sql`

## Common Patterns

### 1. Create with Auto-Generated Fields

```sql
-- name: CreateCrawlJob :one
INSERT INTO crawl_job (
    website_id,
    seed_url,
    priority
) VALUES (
    $1, $2, COALESCE($3, 5)  -- Default priority
)
RETURNING *;
```

### 2. Upsert with ON CONFLICT

```sql
-- name: UpsertContentHash :one
INSERT INTO content_hash (content_hash, first_seen_page_id)
VALUES ($1, $2)
ON CONFLICT (content_hash)
DO UPDATE SET
    occurrence_count = content_hash.occurrence_count + 1,
    last_seen_at = CURRENT_TIMESTAMP
RETURNING *;
```

### 3. Filtered Aggregations

```sql
-- name: GetPageStats :one
SELECT
    COUNT(*) as total_pages,
    COUNT(*) FILTER (WHERE is_duplicate = false) as unique_pages,
    COUNT(*) FILTER (WHERE is_duplicate = true) as duplicate_pages,
    AVG(similarity_score) FILTER (WHERE is_duplicate = true) as avg_similarity
FROM crawled_page
WHERE website_id = $1;
```

### 4. Conditional Updates

```sql
-- name: UpdateCrawlJobStatus :one
UPDATE crawl_job
SET
    status = $2,
    started_at = CASE
        WHEN $2 = 'running' THEN COALESCE($3, CURRENT_TIMESTAMP)
        ELSE started_at
    END,
    completed_at = CASE
        WHEN $2 IN ('completed', 'failed') THEN CURRENT_TIMESTAMP
        ELSE completed_at
    END
WHERE id = $1
RETURNING *;
```

## Best Practices

### 1. Query Organization

Group related queries by domain:
- `website.sql` - Website operations
- `crawl_job.sql` - Job operations
- `crawled_page.sql` - Page operations

### 2. Consistent Naming

Use consistent naming for similar operations:
- `Create{Entity}` - Insert new record
- `Get{Entity}ByID` - Get by primary key
- `Get{Entity}By{Field}` - Get by other field
- `List{Entities}` - Get multiple with pagination
- `Update{Entity}` - Update record
- `Delete{Entity}` - Delete record

### 3. Always Return Data

Use `RETURNING *` for INSERT/UPDATE to get the complete record with auto-generated fields.

### 4. Use Transactions

Always wrap multi-step operations in transactions:

```python
async with session.begin():
    repo = WebsiteRepository(session.connection())
    website = await repo.create(...)
    # More operations...
    # Auto-commits or rolls back
```

### 5. Type Safety

Let sqlc handle types - don't cast manually:

```python
# ✅ Good - sqlc handles conversion
job = await repo.create(
    website_id=str(website.id),
    priority=7
)

# ❌ Bad - unnecessary casting
job = await repo.create(
    website_id=UUID(str(website.id)),  # sqlc expects string
    priority=int(7)  # Already an int
)
```

## Troubleshooting

### Error: "Column not found"

Ensure your SELECT explicitly lists columns or uses `*`:

```sql
-- ❌ Bad
SELECT id, name FROM website;

-- ✅ Good
SELECT * FROM website;
-- or
SELECT id, name, base_url, config, status, created_at, updated_at, created_by
FROM website;
```

### Error: "Parameter count mismatch"

Check that your query parameters match the function signature:

```sql
-- Query expects 3 parameters
INSERT INTO website (name, base_url, config)
VALUES ($1, $2, $3);
```

```python
# Must pass 3 arguments
await website.create_website(conn, "name", "url", {})
```

### Generated Code Not Updating

1. Check `sqlc.yaml` configuration
2. Ensure queries have proper `-- name:` annotations
3. Run `make sqlc-generate` to regenerate
4. Restart Python process to reload modules

## Migration from SQLAlchemy ORM/Core

If migrating from SQLAlchemy:

1. **Keep schema definitions** in `sql/schema/` (no SQLAlchemy table definitions)
2. **Remove** `crawler/db/tables.py` and `crawler/db/base.py` - SQL files are single source of truth
3. **Use repositories** in `crawler/db/repositories.py` for all database operations
4. **Update imports** from SQLAlchemy models to `from crawler.db.generated.models`
5. **Use Pydantic models** from `crawler.db.generated.models` - generated by sqlc from SQL queries

## Testing

Test repositories using the async test fixtures from `tests/conftest.py`:

```python
@pytest.mark.asyncio
async def test_repository(website_repo: WebsiteRepository):
    """Test using repository fixture."""
    website = await website_repo.create(
        name="test",
        base_url="https://test.com",
        config={}
    )
    assert website.name == "test"
```

**Test Fixtures Available:**
- `db_connection` - Raw AsyncConnection for tests
- `website_repo` - WebsiteRepository instance
- `crawl_job_repo` - CrawlJobRepository instance
- `crawled_page_repo` - CrawledPageRepository instance
- `content_hash_repo` - ContentHashRepository instance
- `crawl_log_repo` - CrawlLogRepository instance

All fixtures automatically:
- Create schema from SQL files
- Clean up after tests (automatic schema drop)
- Handle transactions and rollback

## Performance

sqlc-generated code is:
- **Faster than ORMs**: No ORM overhead
- **Type-safe**: Compile-time checks
- **Explicit**: No hidden N+1 queries
- **Lightweight**: Pure SQL with minimal abstraction

Benchmark comparisons show 2-3x performance improvement over SQLAlchemy ORM for typical CRUD operations.

## Further Reading

- [sqlc Documentation](https://docs.sqlc.dev/)
- [sqlc-gen-python Plugin](https://github.com/sqlc-dev/sqlc-gen-python)
- [PostgreSQL 18 UUIDv7](https://www.postgresql.org/docs/18/functions-uuid.html)
